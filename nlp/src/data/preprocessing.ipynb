{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meeting_paths = [\"../../../src/data/023.국회 회의록 기반 지식검색 데이터/1.데이터/Training/02.라벨링데이터\",\n",
    "                 \"../../../src/data/023.국회 회의록 기반 지식검색 데이터/1.데이터/Validation/02.라벨링데이터\"]\n",
    "law_paths = [\"../../../src/data/115.법률-규정 텍스트 분석 데이터_고도화_상황에 따른 판례 데이터/1.데이터/Other\",\n",
    "             \"../../../src/data/115.법률-규정 텍스트 분석 데이터_고도화_상황에 따른 판례 데이터/1.데이터/Training/02.라벨링데이터\",\n",
    "             \"../../../src/data/115.법률-규정 텍스트 분석 데이터_고도화_상황에 따른 판례 데이터/1.데이터/Validation/02.라벨링데이터\"]\n",
    "auto_paths = [\"../../../src/data/143.민원 업무 효율, 자동화를 위한 언어 AI 학습데이터/01.데이터/1.Training/라벨링데이터/TL1\",\n",
    "              \"../../../src/data/143.민원 업무 효율, 자동화를 위한 언어 AI 학습데이터/01.데이터/2.Validation/라벨링데이터/VL1\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 국회 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "df = pd.DataFrame()\n",
    "for path in os.listdir(meeting_paths[0]):\n",
    "        if path == \".DS_Store\":\n",
    "            continue\n",
    "        train_paths =meeting_paths[0] +\"/\"+ path\n",
    "        for file_name in os.listdir(train_paths):\n",
    "            if not file_name.endswith(\".json\"):\n",
    "                continue\n",
    "            file_path = train_paths + \"/\" + file_name\n",
    "            with open(file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                question_data = {\"text\" :[data[\"question\"][\"comment\"]], \"keyword\" : [data[\"question\"][\"keyword\"]]}\n",
    "                answer_data = {\"text\" :[data[\"answer\"][\"comment\"]], \"keyword\" : [data[\"answer\"][\"keyword\"]]}\n",
    "                df = pd.concat([df, pd.DataFrame(question_data)], axis=0, ignore_index=True)\n",
    "                df = pd.concat([df, pd.DataFrame(answer_data)], axis=0, ignore_index=True)\n",
    "                cnt += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"../../../src/data/국회 데이터\")\n",
    "df.to_json('../../../src/data/국회 데이터/train_국회회의록.json', orient = 'index', indent = 4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "df = pd.DataFrame()\n",
    "for path in os.listdir(meeting_paths[1]):\n",
    "        if path == \".DS_Store\":\n",
    "            continue\n",
    "        train_paths =meeting_paths[1] +\"/\"+ path\n",
    "        for file_name in os.listdir(train_paths):\n",
    "            if not file_name.endswith(\".json\"):\n",
    "                continue\n",
    "            file_path = train_paths + \"/\" + file_name\n",
    "            with open(file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                question_data = {\"text\" :[data[\"question\"][\"comment\"]], \"keyword\" : [data[\"question\"][\"keyword\"]]}\n",
    "                answer_data = {\"text\" :[data[\"answer\"][\"comment\"]], \"keyword\" : [data[\"answer\"][\"keyword\"]]}\n",
    "                df = pd.concat([df, pd.DataFrame(question_data)], axis=0, ignore_index=True)\n",
    "                df = pd.concat([df, pd.DataFrame(answer_data)], axis=0, ignore_index=True)\n",
    "                cnt += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('../../../src/data/국회 데이터/val_국회회의록.json', orient = 'index', indent = 4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 민원 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"../../../src/data/민원 데이터\")\n",
    "for path in os.listdir(auto_paths[0]):\n",
    "        if path == \".DS_Store\":\n",
    "            continue\n",
    "        train_paths =auto_paths[0] +\"/\"+ path\n",
    "        files = os.listdir(train_paths)\n",
    "        for file in files:\n",
    "            if not file.endswith(\".json\"):\n",
    "                continue\n",
    "            file_path = train_paths + \"/\" + file\n",
    "            cnt = 0\n",
    "            df = pd.DataFrame()\n",
    "            with open(file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                ingnore_list = []\n",
    "                for doc in data['documents']:\n",
    "                    keywords = \"\"\n",
    "                    for keyword in doc['labeling']['keyword']:\n",
    "                        keywords += keyword['form']\n",
    "                        keywords += \", \"\n",
    "                    keywords = keywords[:-2]\n",
    "                    unified_data = {\"text\" :[doc['Q_refined']], \"keyword\" : [keywords]}\n",
    "                    df = pd.concat([df, pd.DataFrame(unified_data)], axis=0, ignore_index=True)\n",
    "                    cnt += 1\n",
    "                df.to_json(f'../../../src/data/민원 데이터/train_{file}', orient = 'index', indent = 4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in os.listdir(auto_paths[1]):\n",
    "        if path == \".DS_Store\":\n",
    "            continue\n",
    "        train_paths =auto_paths[1] +\"/\"+ path\n",
    "        files = os.listdir(train_paths)\n",
    "        for file in files:\n",
    "            if not file.endswith(\".json\"):\n",
    "                continue\n",
    "            file_path = train_paths + \"/\" + file\n",
    "            cnt = 0\n",
    "            df = pd.DataFrame()\n",
    "            with open(file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                ingnore_list = []\n",
    "                for doc in data['documents']:\n",
    "                    keywords = \"\"\n",
    "                    for keyword in doc['labeling']['keyword']:\n",
    "                        keywords += keyword['form']\n",
    "                        keywords += \", \"\n",
    "                    keywords = keywords[:-2]\n",
    "                    unified_data = {\"text\" :[doc['Q_refined']], \"keyword\" : [keywords]}\n",
    "                    df = pd.concat([df, pd.DataFrame(unified_data)], axis=0, ignore_index=True)\n",
    "                    cnt += 1\n",
    "                df.to_json(f'../../../src/data/민원 데이터/val_{file}', orient = 'index', indent = 4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 법률 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = law_paths[0]\n",
    "sub_paths = os.listdir(path)\n",
    "df = pd.DataFrame()\n",
    "cnt = 0\n",
    "for sub_path in sub_paths:\n",
    "    if sub_path == \".DS_Store\":\n",
    "        continue\n",
    "    file_paths = os.listdir(path + \"/\" + sub_path)\n",
    "    for file_path in file_paths:\n",
    "        address = path + \"/\" + sub_path + \"/\" + file_path\n",
    "        with open(address, \"r\") as f:\n",
    "            a = json.load(f)\n",
    "            data = {\"text\": [a[\"question\"]], \"keyword\" : [a[\"keyword\"]]}\n",
    "            df = pd.concat([df, pd.DataFrame(data)], axis=0, ignore_index=True)\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"../../../src/data/법률 데이터\")\n",
    "df.to_json('../../../src/data/법률 데이터/QA.json', orient = 'index', indent = 4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = law_paths[1]\n",
    "sub_paths = os.listdir(path)\n",
    "df = pd.DataFrame()\n",
    "cnt = 0\n",
    "for sub_path in sub_paths:\n",
    "    if sub_path == \".DS_Store\":\n",
    "        continue\n",
    "    file_paths = os.listdir(path + \"/\" + sub_path)\n",
    "    for file_path in file_paths:\n",
    "        address = path + \"/\" + sub_path + \"/\" + file_path\n",
    "        with open(address, \"r\") as f:\n",
    "            a = json.load(f)\n",
    "            keywords = \"\"\n",
    "            for keyword in a[\"keyword_tagg\"]:\n",
    "                keywords += keyword[\"keyword\"]\n",
    "                keywords += \", \"\n",
    "            keywords = keywords[:-2]\n",
    "            data = {\"text\": [a[\"Summary\"][0][\"summ_contxt\"]], \"keyword\" : [keywords]}\n",
    "            df = pd.concat([df, pd.DataFrame(data)], axis=0, ignore_index=True)\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('../../../src/data/법률 데이터/train_법률-규정.json', orient = 'index', indent = 4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = law_paths[2]\n",
    "sub_paths = os.listdir(path)\n",
    "df = pd.DataFrame()\n",
    "cnt = 0\n",
    "for sub_path in sub_paths:\n",
    "    if sub_path == \".DS_Store\":\n",
    "        continue\n",
    "    file_paths = os.listdir(path + \"/\" + sub_path)\n",
    "    for file_path in file_paths:\n",
    "        address = path + \"/\" + sub_path + \"/\" + file_path\n",
    "        with open(address, \"r\") as f:\n",
    "            a = json.load(f)\n",
    "            keywords = \"\"\n",
    "            for keyword in a[\"keyword_tagg\"]:\n",
    "                keywords += keyword[\"keyword\"]\n",
    "                keywords += \", \"\n",
    "            keywords = keywords[:-2]\n",
    "            data = {\"text\": [a[\"Summary\"][0][\"summ_contxt\"]], \"keyword\" : [keywords]}\n",
    "            df = pd.concat([df, pd.DataFrame(data)], axis=0, ignore_index=True)\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('../../../src/data/법률 데이터/val_법률-규정.json', orient = 'index', indent = 4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 민원 데이터 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../../src/data/민원 데이터\"\n",
    "train_list = [file for file in os.listdir(path) if \"train\" in file]\n",
    "train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for train_file in train_list:\n",
    "    address = path + \"/\" + train_file\n",
    "    print(f\"processing {train_file}...\")\n",
    "    with open(address, \"r\") as f:\n",
    "        df_temp = pd.read_json(f, orient = \"index\")\n",
    "        df = pd.concat([df, df_temp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(f'../../../src/data/민원 데이터/train_민원데이터.json', orient = 'index', indent = 4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../../src/data/민원 데이터\"\n",
    "val_list = [file for file in os.listdir(path) if \"val\" in file]\n",
    "val_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for val_file in val_list:\n",
    "    address = path + \"/\" + val_file\n",
    "    print(f\"processing {val_file}...\")\n",
    "    with open(address, \"r\") as f:\n",
    "        df_temp = pd.read_json(f, orient = \"index\")\n",
    "        df = pd.concat([df, df_temp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(f'../../../src/data/민원 데이터/val_민원데이터.json', orient = 'index', indent = 4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 데이터 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_list = [\"../../../src/data/국회 데이터/train_국회회의록.json\",\n",
    "                    \"../../../src/data/민원 데이터/train_민원데이터.json\",\n",
    "                    \"../../../src/data/법률 데이터/QA.json\",\n",
    "                    \"../../../src/data/법률 데이터/train_법률-규정.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"../../../src/data/legacy\")\n",
    "df = pd.DataFrame()\n",
    "for train_file in total_train_list:\n",
    "    print(f\"processing {train_file}...\")\n",
    "    with open(train_file, \"r\") as f:\n",
    "        df_temp = pd.read_json(f, orient = \"index\")\n",
    "        df = pd.concat([df, df_temp], axis=0, ignore_index=True)\n",
    "df.to_json(f'../../../src/data/legacy/train.json', orient = 'index', indent = 4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_val_list = [\"../../../src/data/국회 데이터/val_국회회의록.json\",\n",
    "                  \"../../../src/data/민원 데이터/val_민원데이터.json\",\n",
    "                  \"../../../src/data/법률 데이터/val_법률-규정.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for val_file in total_val_list:\n",
    "    print(f\"processing {val_file}...\")\n",
    "    with open(val_file, \"r\") as f:\n",
    "        df_temp = pd.read_json(f, orient = \"index\")\n",
    "        df = pd.concat([df, df_temp], axis=0, ignore_index=True)\n",
    "df.to_json(f'../../../src/data/legacy/val.json', orient = 'index', indent = 4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "import torch\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer([\"안녕\", \"바이바이\"], padding=True)[\"input_ids\"][1][2:4] == [7096, 6277]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰 길이 512 넘는 데이터 지우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../../../src/data/legacy/train.json\"\n",
    "val_path = \"../../../src/data/legacy/val.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_path, \"r\") as f:\n",
    "  df_train = pd.read_json(f, orient = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_len = df_train['text'].apply(lambda x : len(tokenizer.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_text_len, bins= range(0, 512, 10), rwidth= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_text_len[train_text_len > 512]) / len(train_text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_len.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deprecated = df_train[train_text_len < 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deprecated.to_json(\"../../../src/data/legacy/train_deprecated.json\", orient = 'index', indent = 4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(val_path, \"r\") as f:\n",
    "  df_val = pd.read_json(f, orient = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text_len = df_val['text'].apply(lambda x : len(tokenizer.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(val_text_len, bins= range(0, 512, 10), rwidth= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_text_len[val_text_len > 512]) / len(val_text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_deprecated = df_val[val_text_len < 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_deprecated.to_json(\"../../../src/data/legacy/val_deprecated.json\", orient = 'index', indent = 4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_text_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 답변 정제(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_json(\"../../../src/data/legacy/train_deprecated.json\", orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train[df_train['keyword'] == \" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['keyword'] != \"\"]\n",
    "df_train = df_train[df_train['keyword'] != \" \"]\n",
    "df_train = df_train[df_train['keyword'] != \"\\n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_len = df_train[\"text\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train_len < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train_len > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_keyword = df_train[\"keyword\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_keyword[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_clean(x:str):\n",
    "  x = x.split(sep = \",\")\n",
    "  l = list()\n",
    "  for e in x:\n",
    "    e = e.strip()\n",
    "    if len(e) == 0:\n",
    "      continue\n",
    "    l.append(e)\n",
    "  return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"keyword\"] = df_train_keyword.apply(split_and_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_string_in_list(df):\n",
    "    return all(item in df['text'] for item in df['keyword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_train[df_train.apply(check_string_in_list, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_len = df_clean['text'].apply(lambda x : len(tokenizer.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_clean_len, bins= range(0, 512, 10), rwidth= 0.5)\n",
    "plt.xlabel(\"Length of input tokens\")\n",
    "plt.ylabel(\"No. data\")\n",
    "plt.title(\"Distribution of train data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 10\n",
    "for i in range(0, 512, interval):\n",
    "  y= len(df_clean[(i < df_clean_len) & (df_clean_len < i + interval)])\n",
    "  print(f\"interval {i}: \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = range(0, 512, 10)\n",
    "df_clean['len'] = pd.cut(df_clean_len, bins=bins, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_max_400(group):\n",
    "    n = min(len(group), 400)\n",
    "    return group.sample(n=n, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = df_clean.groupby(\"len\" , observed=True).apply(sample_max_400).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df[\"len\"] = sampled_df[\"text\"].apply(lambda x : len(tokenizer.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sampled_df['len'], bins = range(0, 512, 10) , rwidth=0.9)\n",
    "plt.xlabel(\"Length of input tokens\")\n",
    "plt.ylabel(\"No. data\")\n",
    "plt.title(\"Distribution of train data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = sampled_df[[\"text\", \"keyword\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.to_json(\"../../../src/data/train_clean.json\", orient = 'index', indent = 4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bio_tags(text, keywords):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    bio_tags = ['O'] * len(tokens)\n",
    "    for keyword in keywords:\n",
    "        keyword_tokens = tokenizer.encode(keyword)[1:-1]\n",
    "        keyword_len = len(keyword_tokens)\n",
    "        if keyword_len == 0:\n",
    "          return tokens, []\n",
    "        for i in range(len(tokens) - keyword_len + 1):\n",
    "            if tokens[i:i + keyword_len] == keyword_tokens:\n",
    "                bio_tags[i] = 'B' \n",
    "                for j in range(1, keyword_len):\n",
    "                    bio_tags[i + j] = 'I'  \n",
    "    return tokens, bio_tags\n",
    "\n",
    "sampled_df[['text_tokens', 'bio_tags']] = sampled_df.apply(\n",
    "    lambda row: pd.Series(create_bio_tags(row['text'], row['keyword'])),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df['keyword_tokens'] = sampled_df['keyword'].apply(lambda x : [tokenizer.encode(e)[1:-1] for e in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = sampled_df[sampled_df[\"bio_tags\"].apply(len) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 답변 정제(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_json(\"../../../src/data/legacy/val_deprecated.json\", orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_val[df_val['keyword'] == \" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_val[df_val['keyword'] != \"\"]\n",
    "df_val = df_val[df_val['keyword'] != \" \"]\n",
    "df_val = df_val[df_val['keyword'] != \"\\n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_len = df_val[\"text\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val[df_val_len < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_val[df_val_len > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_keyword = df_val[\"keyword\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_clean(x:str):\n",
    "  x = x.split(sep = \",\")\n",
    "  l = list()\n",
    "  for e in x:\n",
    "    e = e.strip()\n",
    "    if len(e) == 0:\n",
    "      continue\n",
    "    l.append(e)\n",
    "  return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val[\"keyword\"] = df_val_keyword.apply(split_and_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_string_in_list(df):\n",
    "    return all(item in df['text'] for item in df['keyword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_val[df_val.apply(check_string_in_list, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_len = df_clean['text'].apply(lambda x : len(tokenizer.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_clean_len, bins= range(0, 512, 10), rwidth= 0.5)\n",
    "plt.xlabel(\"Length of input tokens\")\n",
    "plt.ylabel(\"No. data\")\n",
    "plt.title(\"Distribution of test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 10\n",
    "for i in range(0, 512, interval):\n",
    "  y= len(df_clean[(i < df_clean_len) & (df_clean_len < i + interval)])\n",
    "  print(f\"interval {i}: \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = range(0, 512, 10)\n",
    "df_clean['len'] = pd.cut(df_clean_len, bins=bins, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_max_100(group):\n",
    "    n = min(len(group), 100)\n",
    "    return group.sample(n=n, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = df_clean.groupby(\"len\" , observed=True).apply(sample_max_100).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df[\"len\"] = sampled_df[\"text\"].apply(lambda x : len(tokenizer.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sampled_df['len'], bins = range(0, 512, 10) , rwidth=0.9)\n",
    "plt.xlabel(\"Length of input tokens\")\n",
    "plt.ylabel(\"No. data\")\n",
    "plt.title(\"Distribution of test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = sampled_df[[\"text\", \"keyword\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.to_json(\"../../../src/data/test_clean.json\", orient = 'index', indent = 4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bio_tags(text, keywords):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    bio_tags = ['O'] * len(tokens)\n",
    "    for keyword in keywords:\n",
    "        keyword_tokens = tokenizer.encode(keyword)[1:-1]\n",
    "        keyword_len = len(keyword_tokens)\n",
    "        if keyword_len == 0:\n",
    "          return tokens, []\n",
    "        for i in range(len(tokens) - keyword_len + 1):\n",
    "            if tokens[i:i + keyword_len] == keyword_tokens:\n",
    "                bio_tags[i] = 'B'  \n",
    "                for j in range(1, keyword_len):\n",
    "                    bio_tags[i + j] = 'I'  \n",
    "    return tokens, bio_tags\n",
    "\n",
    "sampled_df[['text_tokens', 'bio_tags']] = sampled_df.apply(\n",
    "    lambda row: pd.Series(create_bio_tags(row['text'], row['keyword'])),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df['keyword_tokens'] = sampled_df['keyword'].apply(lambda x : [tokenizer.encode(e)[1:-1] for e in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = sampled_df[sampled_df[\"bio_tags\"].apply(len) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 키워드 개수 통계 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
